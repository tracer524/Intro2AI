# 实验主题

深度学习基础（即大作业的代码任务主要聚焦于：根据给定的输入输出格式，使用Pytorch API设计合理的模型，并完成训练）、博弈基础和强化学习（这两部分已经在 `utils.py` 中完成逻辑实现，不是你的主要任务）

# 实验流程

0. 配置 Torch 环境并截图。如果电脑支持GPU计算，建议同时完成Cuda相关的配置，并开启GPU加速（如果配置成功，训练模型时会打印`current device: cuda.`。如不支持GPU，则会打印`current device: cpu`并随后使用CPU进行模型训练）。另外，还请查看实验文档、参考材料和`readme.md`，来明白本次实验的大体框架。

1. 完成 `submission.py` 并运行 `python submission.py --num_episodes=3000 --checkpoint=500` 来训练模型（根据需要可以将 `3000`和 `500` 更改为更小或更大的值，其中 `num_episodes` 是训练的 episode 数目，checkpoint 是保存操作的周期。比如，设置checkpoint=500，意味着每500个episode就会中途保存一次当前的模型）。

2. 模型训练时，每1000个 episode 会中途保存一次 checkpoint model，中途保存的模型位于 `checkpoint_models` 文件夹中。同时在目录下会生成 `loss_tracker.png`，用来记录 actor 和 critic 的 loss 值，以及 policy 对应的熵。请注意：在Actor-Critic架构中，我们希望 actor loss 能够最大化，而 critic loss 能够最小化。至于 policy 对应的熵意味着什么，还请你进行思考并在实验报告中给出分析。

3. 完成训练后，可以从 `checkpoint_models` 中选择合适的模型进行评测。请自行补全 `model_loader.py` 和 `opponent_loader.py`，使得这两个文件各自的 `get_model()` 和 `get_opponent()` 能够直接加载训练后的模型并返回（不要指定额外的传参）。这里的 `model` 指的是黑棋棋手，而 `opponent` 指的是白棋棋手。模型结构允许不同，因此你也可以在 `model_loader.py` 和 `opponent_loader.py` 自定义待加载的模型结构。

5. 运行 `evaluator.py` 进行评测。如果希望采用随机噪声作为对手，设置 `random_response=True`即可；如果希望采用指定的 opponent 模型作为对手，设置 `random_response=False`即可。

6. 撰写实验报告，并附上评测结果、训练记录（`loss_tracker.png`），进行相关的实验分析。实验中如果遇到（题目要求以外的）bug，请及时记录在报告中，并在报告中给出自己最后的debug方案。

7. 为了探索更优的模型表现，你可以尝试不同的模型结构（cnn，attention机制等）或超参数组合。在记录和分析过程中，请务必记录下实验中最初看似奇怪或难以理解的点。完成实验后，尝试自行解答这些问题，或给出潜在的改进方案。
   
8. 将实验报告整理为pdf文件，与`submission.py`、`model_loader.py` 一并提交。最终提交的模型将和助教的模型（作为opponent）进行对弈，测评结果将占有一定比例的分数。

# 报告要求

请从如下几个部分完成报告：


0. Torch配置；
1. 实验原理（二元零和马尔可夫博弈，Naive Self Play，Actor-Critic等；请用自己的话进行阐述，不要照抄`readme.md`）；
2. 模型设计（附上对应__init__和forward方法中的关键代码），解释这样设计模型的动机和理由；
3. 着重分析constrained policy的解决思路（即 forward 的部分是如何解决置 0 限制和归一限制的）；
4. optimize函数中，3个bug的的解决方案；
5. 在设计模型和处理输入输出时，自己遇到其他bug（比如因为粗心导致张量形状匹配不上，或者in-place操作导致梯度无法计算等等**自己所有可能遇到的问题**）的解决方案；
6. 记录实验中所有最初自己看似奇怪或难以理解的点。完成实验后，尝试自行解答这些问题，或给出潜在的改进方案。
7. 分析 loss 和 entropy 曲线，给出合理的解释。
8. （选做）结合loss的变化和模型的评测表现，进行超参数的选择和模型纵深的调整；或者尝试不同的模型架构（cnn，注意力机制等）
9. （选做）思考题：在最理想的情况下，假设我们的模型的权重能够收敛到使得actor loss和critic loss最优的点。此时黑棋和白棋双方的策略是否一定达成纳什均衡（Nash equilibrium）？为什么？
10. （必做）课程反馈。

备注：其实这次实验保底分应该是比较好拿的，因为代码任务只要求设计基本的模型和正确处理输入输出，实际上也不需要对self-play和强化学习的部分进行代码实现；但是self-play（博弈部分）和RL的部分确实有很多细节和思想（数学上的，假设上的，实验结果上的，or代码上的）隐藏得比较深，而且初看大概率并不会发现有什么问题，只有仔细推敲之后才能明白其中的合理和不合理之处（当然，如果能把这些细节都弄清了，那么你对整个算法框架也就彻底理解了。这时候你对博弈和RL的理解应该会更上一个台阶，但前面仍然有非常远的路要走）。也因此，本次实验的在线issue **原则上不予解答任何原理上的问题**：因为这些问题通常可以分为两类，一类是问“怎么做”（而这类问题在`utils.py` 中是一定可以找到答案的），另一类是“为什么可以这样做”（而这一部分应当是你进行思考的部分。学而不思则殆，不应该直接向助教索求答案而不进行任何思考）。并且，**你的理解水平和思考深度也将是大作业评分标准中一个非常重要的维度。**


# 评分组成

1. 报告规范性（电子版报告的可读性、清晰度、规范度、美观度；特别注意：手写拍照或电子公式排版出现错误均有一定程度的扣分）：???分
2. 代码正确性（能否正常、完整运行，代码逻辑和模型设计是否合理）：???分
3. 报告完整性（上一部分中问题 1-10 的完成程度）、原创性和思考深度：???分
4. 模型表现（和助教模型的对弈结果）：???分（必须保证 `model_loader.py`是正确实现的！如果在助教电脑上不能通过 `model_loader.py`正常加载你的模型并完成对弈，那么你在这一部分是不得分的。助教最终测评用的模型不会公开，如果需要事先评测自己模型的性能，可以和其他同学的模型进行对弈，观察自己模型的表现；或者和随机噪声进行初步对弈。）
5. **请勿抄袭，我们将根据抄袭的严重程度给予抄袭和被抄袭者相对应的惩罚**。

各部分的分数比例会按照最终的实际情况进行调整（但会保证**sum(???)=200**）。你应该尽可能保证质量地完成代码填空和报告的各个部分，并体现自己对整个问题建模、算法框架及理论的理解水平和思考深度，而不应该只根据分数比例投机取巧来获得高分。

# 作业提交

`PB22123456_小明_project.zip`
-— `report.pdf`
-— `model.pth`
-— `model_loader.py`
-— `submission.py`

请确保你训练出来的模型也一并上交，并可以通过`model_loader.py` 被载入对弈当中。


